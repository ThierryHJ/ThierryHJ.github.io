---
title: Development of GPT
author: gourabdg47
date: 25-04-05 02:00:00 +0500
categories: [GenAI, GPT]
tags: [GPT, Reinforcement Learning, RLHF]
render_with_liquid: false
---

Here's a detailed summary of how GPT-1 works, based on its original paper, *Improving Language Understanding by Generative Pre-Training* (Radford et al., 2018):

---

## Overview of GPT-1

GPT-1 (Generative Pre-Trained Transformer) is a foundational model introduced to address the challenge of scarce labeled data in NLP by leveraging large-scale unlabeled corpora for generative pre-training, followed by supervised fine-tuning. This method achieves significant improvements across multiple NLP tasks, such as textual entailment, question answering, semantic similarity, and text classification.

---

## Major Breakthroughs of GPT-1

GPT-1 (Generative Pre-trained Transformer), introduced in the seminal paper *Improving Language Understanding by Generative Pre-Training* (Radford et al., 2018), represents a foundational advancement in NLP. Its primary breakthroughs include:

### 1. **Introducing the Transformer-based Self-supervised Pre-training Paradigm**

Before GPT-1, pre-training methods were successfully used in computer vision but were less systematically applied in NLP. GPT-1's approach was transformative by leveraging self-supervised pre-training, a concept borrowed from computer vision, to NLP. Specifically, GPT-1 first learns general language representations from large unlabeled datasets through generative (language modeling) pre-training and subsequently transfers these learned representations to downstream supervised tasks.

- **Mu Li's Perspective:**
    
    GPT-1 adapted pre-training methods, previously mature in computer vision, to NLP. This shift enabled the effective utilization of large-scale unlabeled textual data, significantly improving performance across multiple NLP tasks. It learns massively on unlabeled text, then fine tuned on labeled text.
    

### 2. **Using a Generative Language Modeling Objective for Pre-training**

GPT-1 utilized a straightforward but powerful training objective: predicting the next word given previous words (causal language modeling). This allowed the model to efficiently learn semantic and syntactic structures directly from unlabeled data.

- **Why is this groundbreaking?**
    
    Previously, NLP tasks heavily relied on supervised learning, requiring expensive and scarce labeled data. GPT-1's generative pre-training dramatically alleviated the reliance on labeled datasets.
    

### 3. **Minimal Architectural Modifications Across Tasks**

GPT-1 emphasized simplicity and generalizability by requiring only task-specific input adaptations (e.g., concatenating inputs with delimiter tokens) rather than extensive architecture customizations for each downstream task. The Transformer decoder architecture remains unchanged during fine-tuning.

- **Mu Li’s Perspective:**
    
    GPT-1's design allows different downstream tasks to be addressed by merely altering the input formatting rather than retraining or redesigning the entire model architecture.
    

## Models

GPT-1 employs a two-stage training procedure:

### 1. Unsupervised Pre-training

- **Objective:**
    
    Uses language modeling (predicting the next word given a context) on a large unlabeled corpus (BooksCorpus, consisting of ~7,000 unpublished books).
    
- **Language Modeling Objective:**
    
    $L_1(U) = \sum_i \log P(u_i|u_{i-k}, \dots, u_{i-1}; \Theta)$
    
    - k is the context window size.
    - Θ represents model parameters.
    - Modeled using a multi-layer Transformer decoder.
- **Model Architecture (Transformer Decoder):**
    
    GPT-1 adopts the Transformer architecture (specifically, the decoder block):
    
    - Multi-headed self-attention mechanisms.
    - Position-wise feed-forward neural networks.
    - Learned position embeddings instead of sinusoidal position encodings.
    - Byte-Pair Encoding (BPE) for tokenization.
- **Training Details:**
    - 12-layer Transformer decoder.
    - 768-dimensional hidden states; 12 attention heads.
    - Adam optimizer with a learning rate warm-up and cosine annealing.
    - Regularization with dropout (rate=0.1), layer normalization, GELU activation, and modified L2 regularization.

---

### 2. Supervised Fine-tuning

![image.png](attachment:e8adeb6f-6ecd-42e4-93e2-ba4cd9762451:image.png)

After unsupervised pre-training, the model is fine-tuned on labeled datasets for specific NLP tasks.

- **Fine-tuning Objective:**

$L_2(C) = \sum_{(x,y)} \log P(y|x_1,\dots,x_m)$

- **Auxiliary Language Modeling Objective:**
    
    Adding a scaled version of the unsupervised objective during fine-tuning can enhance performance:
    

$L_3(C) = L_2(C) + \lambda \times L_1(C)$

- **Minimal Architectural Changes:**
Only minimal adjustments, like adding an additional linear output layer for classification and delimiter tokens embeddings, are necessary.

---

### 3. Task-specific Input Transformations

GPT-1 adapts structured input from different tasks into contiguous token sequences to maintain uniform input processing without significant architecture modifications. Examples:

- **Textual Entailment:** Concatenate premise and hypothesis with a delimiter token.
- **Semantic Similarity:** Concatenate two sentences in both possible orders and sum their representations element-wise.
- **Question Answering:** Concatenate document, question, and possible answers separately for each answer choice.

---

## Evaluation and Results

Evaluated on a variety of benchmarks across several NLP tasks:

| Task Category | Datasets and Examples | Results Highlights |
| --- | --- | --- |
| Natural Language Inference (NLI) | SNLI, MNLI, SciTail, QNLI, RTE | Outperformed previous best models (e.g., MNLI: +1.5%, QNLI: +5.8%) |
| Question Answering & Reasoning | RACE, Story Cloze | Significant gains (RACE: +5.7%, Story Cloze: +8.9%) |
| Semantic Similarity | MRPC, QQP, STS-B | State-of-the-art results (QQP: +4.2%, STS-B: +1%) |
| Text Classification | CoLA, SST-2 | Strong performance on linguistic acceptability (CoLA: +10.4%) and sentiment analysis (SST-2: 91.3%) |

Overall, GPT-1 achieved new state-of-the-art results on 9 out of 12 tasks tested.

---

## Ablation and Analysis Insights

- **Layer Transfer:** Transferring more layers from pre-training to fine-tuning significantly improves performance, showing the effectiveness of learned representations.
- **Zero-shot Performance:** Demonstrated useful linguistic and semantic capabilities acquired solely through unsupervised pre-training.
- **Transformer Advantage:** Transformer architecture significantly outperformed LSTM-based approaches due to better modeling of long-range dependencies.

---

## How GPT-1 Differs from Regular Large Transformers and BERT

While GPT-1 builds upon the Transformer architecture, there are critical distinctions that set it apart:

### 1. **Unidirectional vs. Bidirectional Context**

- **GPT-1 (Unidirectional):**
    - Uses a Transformer **decoder** architecture.
    - Predicts the next token given all previous tokens.
    - Ideal for generative tasks (e.g., text generation).
- **BERT (Bidirectional):**
    - Employs a Transformer **encoder** architecture.
    - Pre-trained with masked language modeling, allowing context from both directions (left and right).
    - Better suited for discriminative tasks (e.g., question answering, natural language inference).

**Implication:**

GPT-1 naturally excels at generation due to its causal modeling, whereas BERT performs better at tasks that require deep bidirectional context understanding.

### 2. **Training Objectives**

- **GPT-1 (Generative):**
    - Language modeling: Predict the next word from previous words.
    - The simplicity of this objective makes training straightforward but limits context to previous tokens only.
- **BERT (Masked Language Modeling):**
    - Randomly masks tokens and trains the model to predict masked tokens based on context from both sides.
    - Allows BERT to learn richer contextual embeddings, improving results on many downstream NLP tasks.

### 3. **Model Complexity and Training Scale**

| Model | Layers | Dimensions | Attention Heads | Parameters | Training Data |
| --- | --- | --- | --- | --- | --- |
| GPT-1 | 12 | 768 | 12 | ~110M | BooksCorpus (~7,000 unpublished books) |
| BERT-base | 12 | 768 | 12 | ~110M | BooksCorpus + English Wikipedia (~4× larger) |
| BERT-large | 24 | 1024 | 16 | ~340M | BooksCorpus + English Wikipedia (~4× larger) |

**Implication:**

BERT, trained on larger and more diverse datasets, captures more nuanced language structures, which translates into superior performance across most NLP tasks compared to GPT-1.

---

## Summary of Key Differences (Mu Li's View Integrated):

| Aspect | GPT-1 | BERT |
| --- | --- | --- |
| Architecture | Transformer Decoder | Transformer Encoder |
| Directionality | Unidirectional (causal LM) | Bidirectional (masked LM) |
| Objective | Next word prediction | Masked token prediction |
| Task Adaptation | Minimal architecture change; task-specific inputs | Minimal architecture change; task-specific heads |
| Data Used | BooksCorpus | BooksCorpus + Wikipedia |
| Performance | Strong on generative tasks | Stronger across broad NLP tasks (due to bidirectional context) |

---

## Why GPT-1 is a Major Step Forward (Conclusion):

GPT-1 set a foundational paradigm in NLP by demonstrating that large-scale self-supervised pre-training significantly improves performance, reducing the dependency on expensive labeled datasets. While subsequent models like BERT improved upon GPT-1 through bidirectionality and richer context modeling, GPT-1's breakthrough paved the way for modern pre-trained language models, including GPT-2, GPT-3, and various Transformer-based architectures widely used today (e.g., GitHub Copilot powered by GPT-3).

This innovation fundamentally changed the NLP landscape, transitioning from heavily supervised methods towards models that first learn general language representations from vast unlabeled data, laying the groundwork for today's large-scale language models.

## Conclusion

GPT-1 introduced a generalizable framework for NLP leveraging generative pre-training on large unlabeled data, followed by discriminative fine-tuning. Its transformer-based model demonstrated significant benefits in understanding language across various NLP tasks and highlighted the potential of large-scale unsupervised learning for NLP.

---

This approach laid the foundation for subsequent developments in generative pre-training models, such as GPT-2 and GPT-3, greatly influencing modern NLP practices.